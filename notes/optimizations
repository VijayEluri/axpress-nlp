* traverse through translation_matrix a few steps ahead.  some paths
may never lead to a solution.

  * create deep translation_matrix.  quickly determine if a
    translation will ever lead to a triple with all of the out vars in
    it

* find translations which have an input property that is only
  generated by one or two other translations.  This could help reduce
  the overhead to execute "calls" where one translation is basically
  'calling' another.

* keep track of which translations are used to get to which solutions.
  If a translation has never been used to reach this solution, search
  it last

* switch values_match to a member function of classes to remove lots of ifs

* lookahead on how far along a path a translation might take
  us. ie. dont add a triple with u.inches_str if there are no
  translations that take it as an input and there isn't one in the
  triple set already ... kinda complex ...

* just increased initial depth to 6 (and broke it).  Could instead,
  not do iterative deepening in cases where there is only 1 path being
  explored.  Could also save the state at each search rather than drop
  it each time ...

* some translations such as %time% %date%

* try precompiling what kinds of triples each translation is capable
  of leading to.  Use this to prune which triples are checked.

* is there a different way to think about triples that would make them
    faster?  objects vertex/edge perhaps?

* where are we needlessly searching?

* how can we avoid it?

  * predict best direction to search in?

  * general faster searching/matching?

* allowing literal values to populate output triples should help

* perhaps allow some fast function (str_to_float) to evaluate in the compiler

  * this may not actually speed things up too much ...
